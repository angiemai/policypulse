{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401acb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import hashlib\n",
    "import logging\n",
    "import shutil\n",
    "import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from pinecone import Pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Your document processing libraries\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from pptx import Presentation\n",
    "from odf import text, teletype\n",
    "from odf.opendocument import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d9d699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MONGO_DB_NAME=your_database_name\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "MONGO_COLLECTION_NAME environment variable is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMONGO_DB_NAME environment variable is required\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mongo_collection_name:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMONGO_COLLECTION_NAME environment variable is required\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pinecone_api_key:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPINECONE_API_KEY environment variable is required\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: MONGO_COLLECTION_NAME environment variable is required"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "%env MONGO_DB_NAME=your_database_name\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "        \n",
    "# Load configuration from environment\n",
    "mongo_connection_string = os.getenv('MONGODB_URI')\n",
    "mongo_db_name = os.getenv('MONGO_DB_NAME')\n",
    "mongo_collection_name = os.getenv('MONGO_COLLECTION_NAME')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_index_name = os.getenv('PINECONE_INDEX_NAME')\n",
    "text_field = os.getenv('PINECONE_TEXT_FIELD')\n",
    "\n",
    "# If environment variables are not set, raise an error\n",
    "if not mongo_connection_string:\n",
    "    raise ValueError(\"MONGODB_URI environment variable is required\")\n",
    "if not mongo_db_name:\n",
    "    raise ValueError(\"MONGO_DB_NAME environment variable is required\")\n",
    "if not mongo_collection_name:\n",
    "    raise ValueError(\"MONGO_COLLECTION_NAME environment variable is required\")\n",
    "if not pinecone_api_key:\n",
    "    raise ValueError(\"PINECONE_API_KEY environment variable is required\")\n",
    "if not pinecone_index_name:\n",
    "    raise ValueError(\"PINECONE_INDEX_NAME environment variable is required\")\n",
    "\n",
    "# Directory configuration\n",
    "staging_directory = os.getenv('STAGING_DIRECTORY', '/Users/aimac/Documents/Coding/policy_pulse_app/database_inbox')\n",
    "processed_directory = os.getenv('PROCESSED_DIRECTORY', '/Users/aimac/Documents/Coding/policy_pulse_app/processed_files')\n",
    "  \n",
    "# If it doesn't exist, create processed directory\n",
    "Path(staging_directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(processed_directory).mkdir(parents=True, exist_ok=True)   \n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "chunk_size=int(os.getenv('CHUNK_SIZE', 2000)),\n",
    "chunk_overlap=int(os.getenv('CHUNK_OVERLAP', 400)),\n",
    "length_function=len,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialize_mongodb(mongo_collection_name, mongo_db_name, mongo_connection_string):\n",
    "        \"\"\"Initialize MongoDB connection.\"\"\"\n",
    "    client = MongoClient(mongo_connection_string, server_api=ServerApi('1'))\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cadec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise API connections\n",
    "initialise_mongodb()\n",
    "initialise_pinecone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9b5bd",
   "metadata": {},
   "source": [
    "<!-- Validate required environment variables -->\n",
    "if not pinecone_api_key:\n",
    "raise ValueError(\"PINECONE_API_KEY environment variable is required\")\n",
    "        if not self.mongo_connection_string:\n",
    "            raise ValueError(\"MONGODB_URI environment variable is required\")\n",
    "        if not self.pinecone_index_name:\n",
    "            raise ValueError(\"PINECONE_INDEX_NAME environment variable is required\")\n",
    "        \n",
    "<!-- Initialize connections -->\n",
    "initialize_mongodb()\n",
    "initialize_pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cfca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        # Validate required environment variables\n",
    "        if not self.pinecone_api_key:\n",
    "            raise ValueError(\"PINECONE_API_KEY environment variable is required\")\n",
    "        if not self.mongo_connection_string:\n",
    "            raise ValueError(\"MONGODB_URI environment variable is required\")\n",
    "        if not self.pinecone_index_name:\n",
    "            raise ValueError(\"PINECONE_INDEX_NAME environment variable is required\")\n",
    "        \n",
    "        # Initialize connections\n",
    "        self._initialize_mongodb()\n",
    "        self._initialize_pinecone()\n",
    "    \n",
    "    def _sanitize_id(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize text to create a valid Pinecone ID.\n",
    "        Pinecone IDs must be ASCII and cannot contain certain special characters.\n",
    "        \"\"\"\n",
    "        # Replace non-ASCII characters and special characters with underscores\n",
    "        sanitized = re.sub(r'[^\\w\\-.]', '_', text)\n",
    "        # Remove multiple consecutive underscores\n",
    "        sanitized = re.sub(r'_+', '_', sanitized)\n",
    "        # Remove leading/trailing underscores\n",
    "        sanitized = sanitized.strip('_')\n",
    "        # Ensure it's not empty\n",
    "        if not sanitized:\n",
    "            sanitized = \"document\"\n",
    "        return sanitized\n",
    "        \n",
    "    def _initialize_mongodb(self):\n",
    "        \"\"\"Initialize MongoDB connection.\"\"\"\n",
    "        try:\n",
    "            self.mongo_client = MongoClient(self.mongo_connection_string)\n",
    "            self.mongo_db = self.mongo_client[self.mongo_db_name]\n",
    "            self.mongo_collection = self.mongo_db[self.mongo_collection_name]\n",
    "            \n",
    "            # Test connection\n",
    "            self.mongo_client.admin.command('ping')\n",
    "            logger.info(\"Successfully connected to MongoDB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to MongoDB: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _initialize_pinecone(self):\n",
    "        \"\"\"Initialize Pinecone connection - assumes index already exists.\"\"\"\n",
    "        try:\n",
    "            # Initialize Pinecone (just like your working code)\n",
    "            pc = Pinecone(api_key=self.pinecone_api_key)\n",
    "            \n",
    "            # Get the existing index (no need to create)\n",
    "            self.pinecone_index = pc.Index(self.pinecone_index_name)\n",
    "            \n",
    "            logger.info(f\"Successfully connected to Pinecone index: {self.pinecone_index_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Pinecone: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _generate_content_hash(self, content: str) -> str:\n",
    "        \"\"\"Generate a hash of the content for change detection.\"\"\"\n",
    "        return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    def _load_document(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from document using your exact extraction functions.\"\"\"\n",
    "        return self.extract_text(file_path)\n",
    "    \n",
    "    # Your exact text extraction functions\n",
    "    def extract_text_from_docx(self, file_path):\n",
    "        \"\"\"Function to extract text from DOCX\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        return text\n",
    "\n",
    "    def extract_text_from_pptx(self, file_path):\n",
    "        \"\"\"Function to extract text from PPTX\"\"\"\n",
    "        prs = Presentation(file_path)\n",
    "        text = []\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text.append(shape.text)\n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    def extract_text_from_odp(self, file_path):\n",
    "        \"\"\"Function to extract text from ODP\"\"\"\n",
    "        doc = load(file_path)\n",
    "        text_elements = doc.getElementsByType(text.P)\n",
    "        return \"\\n\".join([teletype.extractText(element) for element in text_elements])\n",
    "\n",
    "    def extract_text_from_pdf(self, file_path):\n",
    "        \"\"\"Function to check if a PDF is scanned and extract text accordingly\"\"\"\n",
    "        # Open the PDF\n",
    "        doc = fitz.open(file_path)\n",
    "        \n",
    "        # Check if the PDF has text\n",
    "        text = \"\"\n",
    "        text_found = False\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            # If page has more than 10 characters, consider it a text PDF\n",
    "            if len(page_text.strip()) > 10:\n",
    "                text_found = True\n",
    "                text += page_text + \"\\n\"\n",
    "        \n",
    "        # If no significant text found, it's likely a scanned PDF\n",
    "        if not text_found:\n",
    "            logger.info(f\"PDF appears to be scanned, applying OCR: {file_path}\")\n",
    "            return self.extract_text_from_scanned_pdf(file_path)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def extract_text_from_scanned_pdf(self, file_path):\n",
    "        \"\"\"Function to extract text from scanned PDFs using OCR\"\"\"\n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(file_path)\n",
    "        \n",
    "        # Apply OCR to each image\n",
    "        text = []\n",
    "        for i, image in enumerate(images):\n",
    "            text.append(pytesseract.image_to_string(image))\n",
    "        \n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    def extract_text(self, file_path):\n",
    "        \"\"\"Main function to extract text based on file extension\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                return self.extract_text_from_pdf(file_path)\n",
    "            elif extension == '.docx':\n",
    "                return self.extract_text_from_docx(file_path)\n",
    "            elif extension == '.pptx':\n",
    "                return self.extract_text_from_pptx(file_path)\n",
    "            elif extension == '.odp':\n",
    "                return self.extract_text_from_odp(file_path)\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    return f.read()\n",
    "            elif extension == '.md':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    return f.read()\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported file type: {extension}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_file(self, file_path: str, metadata: Optional[Dict] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Process a single file using your exact process_file function logic.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file to process\n",
    "            metadata: Optional metadata to attach to all chunks\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            file_path = Path(file_path)\n",
    "            logger.info(f\"Processing: {file_path}\")\n",
    "            \n",
    "            # Extract text using your exact function\n",
    "            text = self.extract_text(file_path)\n",
    "            if not text:\n",
    "                logger.error(f\"No text extracted from {file_path}\")\n",
    "                return False\n",
    "            \n",
    "            # Split text into chunks\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            \n",
    "            # Create metadata using your exact format\n",
    "            base_metadata = {\n",
    "                \"filename\": file_path.name,\n",
    "                \"file_path\": str(file_path),\n",
    "                \"file_type\": file_path.suffix.lower(),\n",
    "                # Format timestamps as ISO format strings\n",
    "                \"created_at\": datetime.datetime.fromtimestamp(os.path.getctime(file_path)).isoformat(),\n",
    "                \"modified_at\": datetime.datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),\n",
    "                **(metadata or {})\n",
    "            }\n",
    "            \n",
    "            # Process each chunk\n",
    "            successful_chunks = 0\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Create a unique chunk metadata (your exact format)\n",
    "                chunk_metadata = base_metadata.copy()\n",
    "                chunk_metadata[\"chunk_id\"] = i\n",
    "                chunk_metadata[\"chunk_text\"] = chunk[:100] + \"...\"  # Preview of the chunk\n",
    "                \n",
    "                # Create document object with sanitized ID\n",
    "                sanitized_filename = self._sanitize_id(file_path.stem)\n",
    "                document = {\n",
    "                    \"id\": f\"{sanitized_filename}_chunk_{i}\",  # Now sanitized!\n",
    "                    \"text\": chunk,  # Raw text\n",
    "                    \"metadata\": chunk_metadata\n",
    "                }\n",
    "                \n",
    "                # Add chunk to both MongoDB and Pinecone\n",
    "                if self._add_chunk_from_document(document):\n",
    "                    successful_chunks += 1\n",
    "                else:\n",
    "                    logger.warning(f\"Failed to add chunk {document['id']}\")\n",
    "            \n",
    "            # Move file to processed directory if all chunks were successful\n",
    "            if successful_chunks == len(chunks):\n",
    "                self._move_to_processed(file_path)\n",
    "                logger.info(f\"Created {len(chunks)} chunks for {file_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"Only {successful_chunks}/{len(chunks)} chunks processed for {file_path}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process file {file_path}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _add_chunk_from_document(self, document: Dict) -> bool:\n",
    "        \"\"\"Add a chunk to MongoDB and Pinecone from your document format.\"\"\"\n",
    "        try:\n",
    "            import json\n",
    "            \n",
    "            doc_id = document['id']\n",
    "            content = document['text']\n",
    "            doc_metadata = document['metadata']\n",
    "            \n",
    "            # Generate content hash for change detection\n",
    "            content_hash = self._generate_content_hash(content)\n",
    "            \n",
    "            # Check if chunk already exists\n",
    "            existing_chunk = self.mongo_collection.find_one({\"id\": doc_id})\n",
    "            if existing_chunk and existing_chunk.get('content_hash') == content_hash:\n",
    "                logger.info(f\"Chunk {doc_id} already exists with same content, skipping\")\n",
    "                return True\n",
    "            \n",
    "            # Prepare metadata exactly like your working code\n",
    "            metadata_dict = {\n",
    "                \"filename\": doc_metadata[\"filename\"],\n",
    "                \"file_type\": doc_metadata[\"file_type\"],\n",
    "                \"chunk_id\": str(doc_metadata[\"chunk_id\"]),\n",
    "                \"preview\": doc_metadata[\"chunk_text\"]  # This is your preview format\n",
    "            }\n",
    "            \n",
    "            # Convert metadata to a JSON string (exactly like your code)\n",
    "            metadata_str = json.dumps(metadata_dict)\n",
    "            \n",
    "            # Prepare MongoDB document (store both dict and string versions)\n",
    "            mongo_doc = {\n",
    "                \"id\": doc_id,\n",
    "                \"text\": content,  # The raw text\n",
    "                \"metadata\": metadata_dict,  # Store as dict in MongoDB for easy querying\n",
    "                \"metadata_str\": metadata_str,  # Also store JSON string version\n",
    "                \"content_hash\": content_hash,\n",
    "                \"full_metadata\": doc_metadata  # Store your complete metadata too\n",
    "            }\n",
    "            \n",
    "            # Store in MongoDB\n",
    "            self.mongo_collection.replace_one(\n",
    "                {\"id\": doc_id}, \n",
    "                mongo_doc, \n",
    "                upsert=True\n",
    "            )\n",
    "            \n",
    "            # Create record for Pinecone integrated embeddings (your exact format)\n",
    "            record = {\n",
    "                \"id\": doc_id,\n",
    "                \"text\": content,  # This field gets automatically embedded by llama-text-embed-v2\n",
    "                \"metadata\": metadata_str  # Metadata as JSON string\n",
    "            }\n",
    "            \n",
    "            # Upsert to Pinecone using integrated embeddings\n",
    "            self.pinecone_index.upsert_records(\"__default__\", [record])\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to add chunk {document.get('id', 'unknown')}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _move_to_processed(self, file_path: str):\n",
    "        \"\"\"Move file to processed directory.\"\"\"\n",
    "        try:\n",
    "            file_name = Path(file_path).name\n",
    "            processed_path = Path(self.processed_directory) / file_name\n",
    "            \n",
    "            # Handle duplicate filenames\n",
    "            counter = 1\n",
    "            while processed_path.exists():\n",
    "                name_parts = Path(file_name).stem, counter, Path(file_name).suffix\n",
    "                processed_path = Path(self.processed_directory) / f\"{name_parts[0]}_{name_parts[1]}{name_parts[2]}\"\n",
    "                counter += 1\n",
    "            \n",
    "            shutil.move(file_path, processed_path)\n",
    "            logger.info(f\"Moved {file_name} to processed directory\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to move file {file_path}: {e}\")\n",
    "    \n",
    "    def process_staging_directory(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Process all files in the staging directory.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with processing statistics\n",
    "        \"\"\"\n",
    "        staging_path = Path(self.staging_directory)\n",
    "        \n",
    "        # Debug: Check if directory exists and list all files\n",
    "        if not staging_path.exists():\n",
    "            logger.error(f\"Staging directory does not exist: {staging_path}\")\n",
    "            return {\"total_files\": 0, \"successful\": 0, \"failed\": 0}\n",
    "        \n",
    "        logger.info(f\"Checking staging directory: {staging_path}\")\n",
    "        all_files = list(staging_path.iterdir())\n",
    "        logger.info(f\"All files in directory: {[f.name for f in all_files if f.is_file()]}\")\n",
    "        \n",
    "        # Find all files to process (your supported extensions)\n",
    "        supported_extensions = ['.pdf', '.docx', '.pptx', '.odp', '.txt', '.md']\n",
    "        files_to_process = [\n",
    "            f for f in staging_path.iterdir() \n",
    "            if f.is_file() and f.suffix.lower() in supported_extensions\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"Supported extensions: {supported_extensions}\")\n",
    "        logger.info(f\"Files with supported extensions: {[f.name for f in files_to_process]}\")\n",
    "        logger.info(f\"Found {len(files_to_process)} supported documents to process\")\n",
    "        \n",
    "        if len(files_to_process) == 0:\n",
    "            logger.warning(\"No supported files found. Make sure you have files with these extensions in the staging directory:\")\n",
    "            logger.warning(f\"Staging directory: {staging_path}\")\n",
    "            logger.warning(f\"Supported extensions: {supported_extensions}\")\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for file_path in tqdm(files_to_process, desc=\"Processing files\"):\n",
    "            if self.process_file(str(file_path)):\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "        \n",
    "        stats = {\n",
    "            \"total_files\": len(files_to_process),\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Processing complete: {stats}\")\n",
    "        return stats\n",
    "    \n",
    "    def search_similar(\n",
    "        self, \n",
    "        query: str, \n",
    "        top_k: int = 5, \n",
    "        include_content: bool = True,\n",
    "        filter_metadata: Optional[Dict] = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for similar document chunks using Pinecone vector search.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query text\n",
    "            top_k: Number of results to return\n",
    "            include_content: Whether to fetch full content from MongoDB\n",
    "            filter_metadata: Optional metadata filters\n",
    "        \n",
    "        Returns:\n",
    "            List of matching document chunks\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Search using simple query method (like your working code)\n",
    "            # For integrated embeddings, pass the text directly\n",
    "            search_results = self.pinecone_index.query(\n",
    "                vector=query,  # This should work with integrated embeddings\n",
    "                top_k=top_k,\n",
    "                include_metadata=True,\n",
    "                filter=filter_metadata\n",
    "            )\n",
    "            \n",
    "            results = []\n",
    "            \n",
    "            for match in search_results['matches']:\n",
    "                import json\n",
    "                \n",
    "                result = {\n",
    "                    'id': match['id'],\n",
    "                    'score': match['score'],\n",
    "                    'metadata_str': match.get('metadata', '{}')  # JSON string from Pinecone\n",
    "                }\n",
    "                \n",
    "                # Parse the JSON string metadata back to dict\n",
    "                try:\n",
    "                    result['metadata'] = json.loads(result['metadata_str'])\n",
    "                except:\n",
    "                    result['metadata'] = {}\n",
    "                \n",
    "                # Optionally fetch full content from MongoDB\n",
    "                if include_content:\n",
    "                    mongo_doc = self.mongo_collection.find_one({\"id\": match['id']})\n",
    "                    if mongo_doc:\n",
    "                        result['text'] = mongo_doc.get('text', '')\n",
    "                        result['full_metadata'] = mongo_doc.get('metadata', {})\n",
    "                else:\n",
    "                    # Use preview from parsed metadata\n",
    "                    result['text_preview'] = result['metadata'].get('preview', '')\n",
    "                \n",
    "                results.append(result)\n",
    "            \n",
    "            logger.info(f\"Found {len(results)} similar chunks for query: {query[:50]}...\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to search for similar documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def delete_file_chunks(self, source_file: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete all chunks associated with a specific source file.\n",
    "        \n",
    "        Args:\n",
    "            source_file: Name of the source file\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Find all chunks for this file\n",
    "            chunks = list(self.mongo_collection.find({\"metadata.filename\": source_file}))\n",
    "            chunk_ids = [chunk['id'] for chunk in chunks]\n",
    "            \n",
    "            if not chunk_ids:\n",
    "                logger.info(f\"No chunks found for file {source_file}\")\n",
    "                return True\n",
    "            \n",
    "            # Delete from MongoDB\n",
    "            mongo_result = self.mongo_collection.delete_many({\"metadata.filename\": source_file})\n",
    "            \n",
    "            # Delete from Pinecone\n",
    "            self.pinecone_index.delete(ids=chunk_ids, namespace=\"__default__\")\n",
    "            \n",
    "            logger.info(f\"Deleted {mongo_result.deleted_count} chunks for file {source_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to delete chunks for file {source_file}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the document collections.\"\"\"\n",
    "        try:\n",
    "            mongo_count = self.mongo_collection.count_documents({})\n",
    "            pinecone_stats = self.pinecone_index.describe_index_stats()\n",
    "            \n",
    "            # Get file statistics\n",
    "            file_stats = list(self.mongo_collection.aggregate([\n",
    "                {\"$group\": {\n",
    "                    \"_id\": \"$metadata.filename\",\n",
    "                    \"chunk_count\": {\"$sum\": 1}\n",
    "                }},\n",
    "                {\"$group\": {\n",
    "                    \"_id\": None,\n",
    "                    \"total_files\": {\"$sum\": 1},\n",
    "                    \"avg_chunks_per_file\": {\"$avg\": \"$chunk_count\"}\n",
    "                }}\n",
    "            ]))\n",
    "            \n",
    "            stats = {\n",
    "                \"mongodb_chunks\": mongo_count,\n",
    "                \"pinecone_vectors\": pinecone_stats.get('total_vector_count', 0),\n",
    "                \"pinecone_index_fullness\": pinecone_stats.get('index_fullness', 0),\n",
    "                \"total_files\": file_stats[0].get('total_files', 0) if file_stats else 0,\n",
    "                \"avg_chunks_per_file\": round(file_stats[0].get('avg_chunks_per_file', 0), 2) if file_stats else 0\n",
    "            }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get stats: {e}\")\n",
    "            return {}\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
